{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8242216,"sourceType":"datasetVersion","datasetId":4889407},{"sourceId":8250339,"sourceType":"datasetVersion","datasetId":4895225},{"sourceId":8261288,"sourceType":"datasetVersion","datasetId":4903407},{"sourceId":8262666,"sourceType":"datasetVersion","datasetId":4904421},{"sourceId":8263234,"sourceType":"datasetVersion","datasetId":4904853},{"sourceId":8532908,"sourceType":"datasetVersion","datasetId":5096472},{"sourceId":8758731,"sourceType":"datasetVersion","datasetId":5262134},{"sourceId":8946358,"sourceType":"datasetVersion","datasetId":5383587},{"sourceId":206001255,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#-- Matching Same Object --\n#-- Predicting Direction ans Speed --\n#-- Predicting Location of lost Objects --","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:32:38.685103Z","iopub.execute_input":"2024-12-07T14:32:38.686021Z","iopub.status.idle":"2024-12-07T14:32:38.691758Z","shell.execute_reply.started":"2024-12-07T14:32:38.685964Z","shell.execute_reply":"2024-12-07T14:32:38.690617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:34:44.031936Z","iopub.execute_input":"2024-12-07T14:34:44.032355Z","iopub.status.idle":"2024-12-07T14:34:44.037595Z","shell.execute_reply.started":"2024-12-07T14:34:44.032321Z","shell.execute_reply":"2024-12-07T14:34:44.036395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Install ultralytics ------------------------------------------------------------------------------------------\n!pip install ultralytics\n\ndisplay.clear_output()\n\nimport ultralytics\nultralytics.checks()\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:34:45.616513Z","iopub.execute_input":"2024-12-07T14:34:45.617167Z","iopub.status.idle":"2024-12-07T14:34:54.907921Z","shell.execute_reply.started":"2024-12-07T14:34:45.617117Z","shell.execute_reply":"2024-12-07T14:34:54.906807Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Imports ----------------------------------------------------------------------------------------------------\nfrom ultralytics import YOLO\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.models import Model\nfrom collections import defaultdict\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport os\nfrom datetime import datetime, timedelta\nimport shutil\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:34:58.397556Z","iopub.execute_input":"2024-12-07T14:34:58.397952Z","iopub.status.idle":"2024-12-07T14:34:58.404672Z","shell.execute_reply.started":"2024-12-07T14:34:58.397914Z","shell.execute_reply":"2024-12-07T14:34:58.403586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Initialize --------------------------------------------------------------------------------------------------\nout_dir = '/kaggle/working/'\ndetection_weights_file = '/kaggle/input/yolo11-11frozen-13/model_11_frozen_epoch_60/train/weights/best.pt'\n\ndrone_files = ['/kaggle/input/drone-dataset-p1/v_5.mp4']#,\n              #'/kaggle/input/drone-dataset-p2/v_8.mp4',\n              #'/kaggle/input/drone-detection-test-videos-1/drone_video (1).mp4',\n              # '/kaggle/input/novin-data/Novin_Dataset/f2.part2.mp4',\n              #'/kaggle/input/sample-videos-detecting-and-matching-objs-1/sample_video_drone (5).mp4',\n              #'/kaggle/input/video-drone-bird-1/Untitled-13.mp4']\n\nresults_dir = out_dir + 'results/'\nif not os.path.exists(results_dir):\n    os.makedirs(results_dir)\n\nAREA_THRESHOLD = 5\nDISTANCE_THRESHOLD = 50\nSIMILARITY_THRESHOLD = 0.6\nCROP_PADDING = 10\nTIME_THRESHOLD = 60\nNUM_TRACK_THRESHOLD = 30\nDIRECTION_HISTORY = 10\nMOVING_THRESHOLD = 1e-2\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:01.400809Z","iopub.execute_input":"2024-12-07T14:35:01.401772Z","iopub.status.idle":"2024-12-07T14:35:01.407997Z","shell.execute_reply.started":"2024-12-07T14:35:01.401717Z","shell.execute_reply":"2024-12-07T14:35:01.406860Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Set Detection Model ------------------------------------------------------------------------------------------\nmodel = YOLO(detection_weights_file)  \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:03.963535Z","iopub.execute_input":"2024-12-07T14:35:03.964605Z","iopub.status.idle":"2024-12-07T14:35:04.416959Z","shell.execute_reply.started":"2024-12-07T14:35:03.964549Z","shell.execute_reply":"2024-12-07T14:35:04.416070Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Set Similarity Measure Model ---------------------------------------------------------------------------------\nsimilarity_base_model = ResNet50(weights='imagenet')\n\n#-- Use the second-last layer for embeddings --\nsimilarity_model = Model(inputs=similarity_base_model.input,\n                         outputs=similarity_base_model.layers[-2].output)  \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:05.607993Z","iopub.execute_input":"2024-12-07T14:35:05.608518Z","iopub.status.idle":"2024-12-07T14:35:07.129434Z","shell.execute_reply.started":"2024-12-07T14:35:05.608464Z","shell.execute_reply":"2024-12-07T14:35:07.128612Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Preprocess Image for Similarity Measure ---------------------------------------------------------\ndef preprocess_image(image, target_size=(224, 224)):\n    \n    image = cv2.resize(image, target_size) \n    image = np.expand_dims(image, axis=0)  #-- Add batch dimension\n    image = preprocess_input(image)  #-- Normalize for ResNet\n    return image\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:08.436754Z","iopub.execute_input":"2024-12-07T14:35:08.437688Z","iopub.status.idle":"2024-12-07T14:35:08.444297Z","shell.execute_reply.started":"2024-12-07T14:35:08.437632Z","shell.execute_reply":"2024-12-07T14:35:08.443324Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Calculate Similarity -----------------------------------------------------------------------------\ndef compare_similarity_images(image1, image2):   \n    \n    #-- Preprocess images --\n    img1 = preprocess_image(image1)\n    img2 = preprocess_image(image2)\n    \n    #-- Extract features --\n    embedding1 = similarity_model.predict(img1)\n    embedding2 = similarity_model.predict(img2)\n\n    #-- Compute cosine similarity --\n    similarity_score = cosine_similarity(embedding1, embedding2)[0][0]     \n\n    return similarity_score\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:09.600686Z","iopub.execute_input":"2024-12-07T14:35:09.601034Z","iopub.status.idle":"2024-12-07T14:35:09.606429Z","shell.execute_reply.started":"2024-12-07T14:35:09.601002Z","shell.execute_reply":"2024-12-07T14:35:09.605346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Match Detected Objects ---------------------------------------------------------------------------\ndef match_object(track_id, track_box, track_image, track_time, last_tracked_objects):   \n    \n    # plt.imshow(track_image)\n    # plt.title(f'track_object - id:{track_id}')\n    # plt.axis('off')  \n    # plt.show()    \n    \n    track_center_x, track_center_y, track_w, track_h = track_box     \n    \n    distance_match = {}\n    similarity_match = {}\n    \n    matched_id = None\n    \n    for obj_id, (obj_box, obj_img, obj_time) in last_tracked_objects.items():\n        \n        print(f'####################### {obj_id} #####################')\n        # plt.imshow(obj_img)\n        # plt.title(f'object- id:{obj_id}')\n        # plt.axis('off')  \n        # plt.show()    \n        \n        time_difference = abs(track_time - obj_time)\n        print(f'-------- time_difference: {time_difference} -------------')\n        if time_difference > timedelta(seconds=TIME_THRESHOLD):            \n            continue\n        \n        similarity_score = compare_similarity_images(track_image, obj_img)\n        print(f'-------- similarity_score: {similarity_score} -------------')\n        if similarity_score < SIMILARITY_THRESHOLD:            \n            continue\n        \n        x_center, y_center, w, h = obj_box     \n        \n        a_track = track_w * track_h\n        a_obj = w *h        \n        if a_track>a_obj:\n            a_ratio = a_track/a_obj\n        else:\n            a_ratio = a_obj/a_track\n        \n        print(f'-------- a_ratio: {a_ratio} -------------')\n        if a_ratio > AREA_THRESHOLD:            \n            continue         \n        \n        distance = np.sqrt((track_center_x - x_center)**2 + (track_center_y - y_center)**2)      \n        print(f'-------- distance: {distance} -------------')\n        if distance <= DISTANCE_THRESHOLD:            \n            distance_match[obj_id] = distance\n        else:\n            similarity_match[obj_id] = similarity_score\n    \n    if len(distance_match)!=0:\n        matched_id = min(distance_match, key=distance_match.get) \n    elif len(similarity_match)!=0:\n        matched_id = min(similarity_match, key=similarity_match.get)       \n    \n    print(f'-------- matched_id: {matched_id} -------------')\n    return matched_id\n\n      \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:10.906421Z","iopub.execute_input":"2024-12-07T14:35:10.907200Z","iopub.status.idle":"2024-12-07T14:35:10.916421Z","shell.execute_reply.started":"2024-12-07T14:35:10.907158Z","shell.execute_reply":"2024-12-07T14:35:10.915381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function To Crop Object Using Bounding Box ---------------------------------------------------------------\ndef crop_object(frame, box, padding=CROP_PADDING):\n    \n    frame_height, frame_width = frame.shape[:2]    \n    center_x, center_y, w, h = box  \n    \n    top_left_x = int(max(center_x - w // 2 - padding, 0))\n    top_left_y = int(max(center_y - h // 2 - padding, 0))\n    bottom_right_x = int(min(center_x + w // 2 + padding, frame_width))\n    bottom_right_y = int(min(center_y + h // 2 + padding, frame_height))\n    \n    cropped_object = frame[top_left_y:bottom_right_y, top_left_x:bottom_right_x].copy()\n    \n    return cropped_object\n#-----------------------------------------------------------------------------------------------------------------","metadata":{"execution":{"iopub.status.busy":"2024-12-07T14:35:13.273215Z","iopub.execute_input":"2024-12-07T14:35:13.273936Z","iopub.status.idle":"2024-12-07T14:35:13.279721Z","shell.execute_reply.started":"2024-12-07T14:35:13.273896Z","shell.execute_reply":"2024-12-07T14:35:13.278729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_direction(track_history, track_id, num_frames=DIRECTION_HISTORY, threshold=MOVING_THRESHOLD):\n    track = track_history.get(track_id, [])\n    if len(track) < num_frames:\n        return None, (0, 0)  # Not enough data to predict direction  \n\n    # Check if object is stationary\n    deltas = []\n    for i in range(1, len(track)):\n        delta_x = track[i][0] - track[i-1][0]\n        delta_y = track[i][1] - track[i-1][1]\n        deltas.append(np.sqrt(delta_x**2 + delta_y**2))  # Euclidean distance\n\n    avg_delta = np.mean(deltas[-num_frames:])\n    if avg_delta < 5:\n        return \"Stationary\", (0, 0)  \n    \n    # Predict Direction\n    delta_x = track[-1][0] - track[-num_frames][0]\n    delta_y = track[-1][1] - track[-num_frames][1]\n\n    # Normalize the direction vector\n    magnitude = np.sqrt(delta_x**2 + delta_y**2)\n    if magnitude > 0:\n        delta_x /= magnitude\n        delta_y /= magnitude\n\n    if abs(delta_x) < threshold and abs(delta_y) < threshold:\n        return \"Stationary\", (0, 0)\n\n    directions = {\n        (-1, -1): \"Up-Left\", (-1, 0): \"Left\", (-1, 1): \"Down-Left\",\n        (0, -1): \"Up\", (0, 0): \"Stationary\", (0, 1): \"Down\",\n        (1, -1): \"Up-Right\", (1, 0): \"Right\", (1, 1): \"Down-Right\",\n    }\n    dx_sign = np.sign(delta_x)\n    dy_sign = np.sign(delta_y)\n    direction = directions.get((dx_sign, dy_sign), \"Unknown\")\n    return direction, (delta_x, delta_y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:35:14.842470Z","iopub.execute_input":"2024-12-07T14:35:14.843374Z","iopub.status.idle":"2024-12-07T14:35:14.857079Z","shell.execute_reply.started":"2024-12-07T14:35:14.843309Z","shell.execute_reply":"2024-12-07T14:35:14.856055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Calculate Speed of Movement --------------------------------------------------------------------all\ndef calculate_speed(track_history, track_id, fps):\n    \n    track = track_history.get(track_id, [])\n    if len(track) >= 2:  #-- At least two points are required\n        x1, y1 = track[-2]  #-- Previous coordinates\n        x2, y2 = track[-1]  #-- Current coordinates\n        \n        distance = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\n        \n        speed = distance #* fps  #-- Pixels per second\n        return speed\n    return 0  \n#-----------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:35:16.831200Z","iopub.execute_input":"2024-12-07T14:35:16.831597Z","iopub.status.idle":"2024-12-07T14:35:16.837776Z","shell.execute_reply.started":"2024-12-07T14:35:16.831551Z","shell.execute_reply":"2024-12-07T14:35:16.836582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Function to Calculate Arrow of Direction ------------------------------------------------------------------\ndef get_arrow(x, y, delta_x, delta_y):\n    \n    start_point = (int(x), int(y))  #-- center of bounding box                             \n                            \n    arrow_scale = 50 \n    normalized_dx = delta_x / max(abs(delta_x), abs(delta_y), 1e-5)  \n    normalized_dy = delta_y / max(abs(delta_x), abs(delta_y), 1e-5)                            \n                            \n    end_point = (int(x + normalized_dx * arrow_scale),\n                 int(y + normalized_dy * arrow_scale))\n\n    return start_point, end_point\n#--------------------------------------------------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:35:18.639377Z","iopub.execute_input":"2024-12-07T14:35:18.639728Z","iopub.status.idle":"2024-12-07T14:35:18.645835Z","shell.execute_reply.started":"2024-12-07T14:35:18.639698Z","shell.execute_reply":"2024-12-07T14:35:18.644716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def is_object_exiting(frame_width, frame_height, bbox, direction_vector, margin=10):\n    \n    x_min, y_min, x_max, y_max = bbox\n    delta_x, delta_y = direction_vector\n    \n    \n    near_left = x_min <= margin\n    near_right = x_max >= frame_width - margin\n    near_top = y_min <= margin\n    near_bottom = y_max >= frame_height - margin\n    \n    \n    moving_left = delta_x < 0 and near_left\n    moving_right = delta_x > 0 and near_right\n    moving_up = delta_y < 0 and near_top\n    moving_down = delta_y > 0 and near_bottom\n    \n    \n    return moving_left or moving_right or moving_up or moving_down\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T14:35:20.411929Z","iopub.execute_input":"2024-12-07T14:35:20.412326Z","iopub.status.idle":"2024-12-07T14:35:20.418642Z","shell.execute_reply.started":"2024-12-07T14:35:20.412293Z","shell.execute_reply":"2024-12-07T14:35:20.417625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def update_bbox_position(bbox, speed, delta_x, delta_y):\n    x, y, w, h = bbox\n\n    normalized_dx = delta_x / max(abs(delta_x), abs(delta_y), 1e-5)  \n    normalized_dy = delta_y / max(abs(delta_x), abs(delta_y), 1e-5)   \n    \n    # Calculate the new position based on velocity and direction\n    # new_x = x + (speed/10) * normalized_dx\n    # new_y = y + (speed/10) * normalized_dy\n\n    speed = speed/3\n    new_x = x + speed * normalized_dx\n    new_y = y + speed * normalized_dy\n\n    # Return the updated bounding box\n    return new_x, new_y, w, h\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T15:01:25.950783Z","iopub.execute_input":"2024-12-07T15:01:25.951851Z","iopub.status.idle":"2024-12-07T15:01:25.959178Z","shell.execute_reply.started":"2024-12-07T15:01:25.951785Z","shell.execute_reply":"2024-12-07T15:01:25.958050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#-- Run ----------------------------------------------------------------------------------------------------------\nfor video_file in drone_files:\n    \n    #-- get video name --\n    index = video_file.rfind('/')      \n    video_name = video_file[index + 1:] \n    \n    #-- set output file --\n    out_video_name = 'out_' + video_name    \n    output_path = results_dir + out_video_name\n    \n    print(f'=== Processing {video_name} ================================')\n    \n    cap = cv2.VideoCapture(video_file)\n    \n    #-- get video properties --\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    #-- set video writer --\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  \n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    \n    track_history = defaultdict(lambda: []) #-- for tracking\n    last_tracked_objects = {} #-- for matching  \n    last_pred_objects = {} \n    last_ids = set() #-- set of last ids     \n    last_status = {} #-- \n    last_speed = {}\n    mapped_objects = {}\n    frame_number = 0\n    \n    while cap.isOpened():    \n        success, frame = cap.read()\n        if success:            \n            frame_number += 1\n            print(f'\\nframe number = {frame_number} ==================================') \n            print('track_history:', track_history.keys())\n            print('last_tracked_objects:', last_tracked_objects.keys())\n\n            for k,v in last_tracked_objects.items():\n                print(f'----{k}-{v[0]} ')\n\n            \n            # if frame_number>=1000:\n            #     break            \n\n            current_ids = set() #-- set of ids in the cuurent framse\n            \n            #-- detect and track objects --\n            results = model.track(frame,    \n                                  tracker = 'bytetrack.yaml',\n                                  persist=True,\n                                  show = False)\n\n            #-- Check if there are any detections --\n            if results[0].boxes is not None and results[0].boxes.xywh is not None:            \n                boxes = results[0].boxes.xywh.cpu()\n                track_ids = results[0].boxes.id\n                if track_ids is not None:\n                    track_ids = track_ids.int().cpu().tolist()       \n                                \n                    for box, track_id in zip(boxes, track_ids):     \n\n                        #-- Crop the object from the frame --\n                        cropped_object = crop_object(frame, box)\n                        detection_time = datetime.now()\n                        \n                        #-- Check if this is the first detected object --\n                        if len(last_tracked_objects)==0:\n                            last_tracked_objects[track_id] = (box, cropped_object, detection_time)\n                        else:\n                            #-- Check if its not new detected object --\n                            if track_id in last_tracked_objects:\n                                last_tracked_objects[track_id] = (box, cropped_object, detection_time)                   \n                                \n                            else:   \n                                if track_id in mapped_objects:\n                                    matched_id =  mapped_objects[track_id]\n                                else:\n                                    matched_id = match_object(track_id, box, cropped_object, detection_time, last_tracked_objects)\n                                if matched_id is not None:\n                                    mapped_objects[track_id] = matched_id\n                                    track_id = matched_id                                    \n                                last_tracked_objects[track_id] = (box , cropped_object, detection_time)   \n                                \n                        \n                        current_ids.add(track_id)\n                        last_ids.add(track_id)\n\n                        if track_id in last_pred_objects:\n                            del last_pred_objects[track_id]\n                        \n                        \n                                \n                        #-- track --                  \n                        annotated_frame = frame\n                        x, y, w, h = box\n                        track = track_history[track_id]\n                        track.append((float(x), float(y)))  #-- x, y center point\n                        if len(track) > NUM_TRACK_THRESHOLD:  #-- retain NUM_TRACK_THRESHOLD tracks\n                            track.pop(0)\n\n                        #-- Draw the tracking lines --\n                        points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n                        cv2.polylines(annotated_frame, [points], isClosed=False, color=(0, 0, 255), thickness=4)\n                        \n                        #-- Draw the bounding box --\n                        top_left = (int(x - w / 2), int(y - h / 2))\n                        bottom_right = (int(x + w / 2), int(y + h / 2))\n                        cv2.rectangle(annotated_frame, top_left, bottom_right, (255, 0, 0), 2)  #-- Blue bounding box\n\n                        #-- Put the ID text --\n                        text_position = (int(x - w / 2), int(y - h / 2) - 10)\n                        cv2.putText(annotated_frame, f'ID: {track_id}', text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n                        #-- Calculate Speed and Direction --\n                        speed = calculate_speed(track_history, track_id, fps)\n                        direction, (delta_x, delta_y) = predict_direction(track_history, track_id)\n\n                        last_speed[track_id] = (speed, (delta_x, delta_y))\n\n                        if direction != \"Stationary\": \n                            last_status[track_id] = is_object_exiting(frame_width,\n                                                                  frame_height,\n                                                                  box,\n                                                                  (delta_x, delta_y))\n                        else:\n                            last_status[track_id] = False\n\n                        #-- Calculating the coordinates of the bounding box --\n                        top_left_x = int(x - w / 2)\n                        top_left_y = int(y - h / 2)\n                        bottom_right_x = int(x + w / 2)\n                        bottom_right_y = int(y + h / 2)\n\n                        if direction == \"Stationary\":                            \n                            text_position = (top_left_x, bottom_right_y + 20)  \n                            cv2.putText(frame, direction, text_position, \n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n                        else:\n                            start_point, end_point = get_arrow(x, y, delta_x, delta_y)\n                            \n                            #-- Draw Arrow on the frame --\n                            cv2.arrowedLine(annotated_frame, start_point, end_point, (255, 0, 0), 2, tipLength=0.5)\n                    \n                            #-- Put the direction and speed under the bounding box --\n                            text_position = (top_left_x, bottom_right_y + 20)  \n                            cv2.putText(frame, f'Dir: {direction}, Speed: {speed:.2f}px/s', text_position, \n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)                           \n                        \n                else:\n                    annotated_frame = frame  #-- If no IDs, use original frame\n\n            else:\n                annotated_frame = frame  #-- If no boxes, use original frame\n\n            print('############################################')\n            for k,v in last_tracked_objects.items():\n                print(f'----{k}-{v[0]} ')\n            print('############################################')\n            \n            for id in last_ids:\n                # if id not in current_ids and last_status[id]:\n                #     x , y , w, h = last_tracked_objects[id][0] #-- box                    \n                #     top_left = (int(x - w / 2), int(y - h / 2))\n                #     bottom_right = (int(x + w / 2), int(y + h / 2))\n                #     cv2.rectangle(annotated_frame, top_left, bottom_right, (255, 255, 255), 2)  \n\n                \n                    \n                    \n                if id not in current_ids and not last_status[id]:\n                    if id not in last_pred_objects:\n                        last_box = last_tracked_objects[id][0]\n                    else:\n                        last_box = last_pred_objects[id][0]                        \n                        \n                    new_box = update_bbox_position(bbox= last_box,\n                                                   speed= last_speed[id][0],\n                                                   delta_x= last_speed[id][1][0],\n                                                   delta_y= last_speed[id][1][1])\n                    print('**************************', id)\n                    print('last_box:' , last_box)\n                    print('pred_box:' , new_box)\n                    print('*****************************************')\n\n                    cropped_object = last_tracked_objects[id][0]\n                    detection_time = datetime.now()\n                    last_pred_objects[id] = (new_box, cropped_object, detection_time)\n                    \n                    #-- Draw the bounding box --\n                    #x , y , w, h = last_tracked_objects[id][0] #-- box\n                    x , y , w, h = new_box\n                    top_left = (int(x - w / 2), int(y - h / 2))\n                    bottom_right = (int(x + w / 2), int(y + h / 2))\n                    cv2.rectangle(annotated_frame, top_left, bottom_right, (255, 255, 0), 2)  \n\n                    #-- track --  \n                    track = track_history[id]\n                    track.append((float(x), float(y)))  #-- x, y center point\n                    if len(track) > NUM_TRACK_THRESHOLD:  #-- retain NUM_TRACK_THRESHOLD tracks\n                        track.pop(0)\n\n                    #-- Draw the tracking lines --\n                    points = np.array(track, dtype=np.int32).reshape((-1, 1, 2))\n                    cv2.polylines(annotated_frame, [points], isClosed=False, color=(255, 255, 0), thickness=4)  \n\n                    #-- Put the ID text --\n                    text_position = (int(x - w / 2), int(y - h / 2) - 10)\n                    cv2.putText(annotated_frame, f'ID: {id}', text_position, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n                                  \n\n                    \n                    \n\n            out.write(annotated_frame)\n\n        else:\n            #-- Break the loop if the end of the video is reached --\n            break\n\n    #-- Release the video capture object and close the display window --\n    cap.release()\n    out.release()   \n    #display.clear_output()\n    ","metadata":{"execution":{"iopub.status.busy":"2024-12-07T15:01:28.846719Z","iopub.execute_input":"2024-12-07T15:01:28.847244Z","iopub.status.idle":"2024-12-07T15:02:36.033027Z","shell.execute_reply.started":"2024-12-07T15:01:28.847162Z","shell.execute_reply":"2024-12-07T15:02:36.031754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# zip_results = \"results\"\n# shutil.make_archive(zip_results, 'zip', results_dir)","metadata":{"execution":{"iopub.status.busy":"2024-12-04T13:37:40.273782Z","iopub.execute_input":"2024-12-04T13:37:40.274051Z","iopub.status.idle":"2024-12-04T13:37:40.277645Z","shell.execute_reply.started":"2024-12-04T13:37:40.274026Z","shell.execute_reply":"2024-12-04T13:37:40.27686Z"},"trusted":true},"outputs":[],"execution_count":null}]}